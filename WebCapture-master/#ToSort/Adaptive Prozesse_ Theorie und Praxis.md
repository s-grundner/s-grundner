# Adaptive Prozesse: Theorie und Praxis

_Captured: 2015-09-23 at 12:04 from [adaptiveprocess.wordpress.com](https://adaptiveprocess.wordpress.com/2010/05/31/adaptive-prozesse-theorie-und-praxis/)_

Die Ideen und Konzepte rund um adaptive Prozesse sind absolut nichts Neues. Mein Bekanntheitsgrad wie ein bunter Hund kommt daher, dass ich seit zehn Jahren lautstark bis provokativ gegen die Idee protestiere, dass man Unternehmen mittels BPM perfekt strukturieren und damit verbessern kann. Sie werden damit zu Tode organisiert. Ich finde auch, dass man den Terminus 'ADAPTIV' fur BPM falsch verwendet. Er wird jedoch nun zum nachsten Schlagwort im 'IT Buzzword Bingo' Spiel. Hersteller die simples, orthodoxes BPM anbieten schmucken sich nun mit dem neuen Adjektiv. Ach, wie hubsch! Wie bei Sex ist es ein substantieller Unterschied daruber zu reden oder es zu tun. Zum Gluck ist es nicht unmoralisch (wie etwa bei Sex) die großspurigen Behauptungen von Anbietern auszutesten. Sie sollten daher jede Software intensiv probefahren (naturlich auch [Papyrus](http://www.isis-Papyrus.com) …) bevor sie sich entscheiden.

1997 begann ich meine Theorien zu 'Adaptive Applications' in die Praxis umzusetzen mit der Entwicklung der Papyrus Platform (damals noch Objects genannt), die dann in 2000 auch erstmals installiert wurde. Schon davor gab es jedoch akademische Forschung zu adaptiven Prozessen von Manfred Reichert and Peter Dadam et al. an der Universitat Ulm mit dem ADEPT Konzept, sowie zu Process Mining von Prozesslogs durch Christian W. Gunther and Wil M.P. van der Aalst an der Eindhoven University of Technology. Die theoretische Arbeit an den Universitaten zeigt auf, dass es seit damals kritische Denker zu starren Prozessen gibt und nicht nur ich meinen kontroversen Ideen nachlaufe. Dennoch gibt es Unterschiede zwischen der theoretischen Forschung und den realen Problemen in Unternehmen. Diese mochte ich hier diskutieren.

ADEPT1 von 1998 fokussierte auf Ad-Hoc Flexibilitat und die verteilte Ausfuhrung von Prozessen, mit der auch Prozessanderungen an bereits laufende Instanzen weitergeleitet werden. Die Ad-Hoc Erstellung und Änderung wurde mittels formellen Änderungsfunktionen mit Pre- und Post-Constraints erreicht. Die Limitationen der Prozessstruktur sind damit nicht ein Nachteil sondern so gewollt. Ich stelle jedoch den praktischen Nutzen der formellen Richtigkeit in Frage, da es bei Prozessen immer zu Ausnahmen kommen kann und deren Losung ist kein Fehler sondern normale Prozessarbeit. Bei ADEPT2 wurde vor allem eine unabhangige Datenstruktur eingefuhrt, die Geschaftsdaten als versionierte Objekte definiert und damit die transaktionelle Richtigkeit von Daten sicherstellt. ADEPT2 ist eine theoretische Losung fur die Adaptierung von Flussdiagrammen, die jedoch sich mit einigen praktischen Aspekten nicht auseinandersetzt. Ich gestehe, dass ich den allerletzten Stand der Arbeiten nicht kenne.

Was sind nun die Unterschiede zu meiner Forschungs- und Entwicklungsarbeit. ADPET2 berucksichtigt die Adaptierbarkeit der notwendigen Prozessressourcen, wie eingehende und ausgehende Dokumente, Geschaftsregeln, und Benutzeroberflachen nicht. Flexibiitaten, wie das freie Hinzufugen von Teilnehmern, das Einfugen von Meilensteinen, Regeln und Dokumenten erfordert weiters eine starke Authorisierungsfunktion uber Rollenmodelle. Einige Eigenschaften von ADEPT2 - modellausfuhrend und objektorientiert - werden in BPMN System wie Papyrus gefunden. Es gibt auch eine kommerzielle Version von ADEPT2. Papyrus bildet auch das Prozessmodell auf Basis von frei-definierbaren Objekten ab und alle Datenobjekte haben ausserdem eine prozessunabhangige Existenz und konnen in vielen Prozessen genutzt werden. Weiters nutzt das Taskmodell von ADEPT2 eine starre State-Engine und in Papyrus ist diese frei definierbar. Prozesse konnen aus beliebig vielen Elementen bestehen die mit State/Events und Regeln vernetzt sind. Das Flussdiagramm ist nur eine Sicht aber nicht die logische Struktur des Prozesses.

Wie sieht es mit den Forschungen zu Process-Mining von Christian W. Gunther and Wil M.P. van der Aalst an der Eindhoven University of Technology aus? Liefern diese mehr realitatsbezogene Losungen? Die erste Frage die man stellen muss ist, von wo man die Prozesslogs bekommt? Die Logs eines ublichen BPM Systems liefern keine Information weil diese ja schon starr sind und daher nur die Haufigkeit von Gateway-Entscheidungen liefern wurde. Bei Ad-Hoc oder dynamischem BPM bekommt man mehr Informationen, aber diese sind auch schon eingeschrankt um damit Prozesse aus der Realitat zu ermitteln. Eigentlich mußten Email oder Soziale Netzwerke analysiert werden. Aalst verfolgte auch eine Losung bei der Process Mining die Änderungsvorschlage liefert und ADEPT 2 die Funktion. Aalst schrieb dazu: "that process mining, in order to become more meaningful, and to become applicable in a wider array of practical settings, needs to address the problems it has with unstructured processes."

![](https://adaptiveprocess.files.wordpress.com/2010/05/process-mining-and-adaptive.png?w=500&h=438)

> _ADEPT2 und Process-Mining (Quelle: W. Van Der Aalst)_

Wenn Benutzer jedoch frei agieren konnen fuhrt dies zu einer weiteren Dynamik die in der Analyse als 'Spaghetti-Diagramme' sichtbar werden. Diese zeigen zwar die Realitat, erlauben aber nicht die Richtung eines Prozessflusses oder die kausalen Zusammenhange zu ermitteln, da jeder Event-Knoten mit jedem anderen irgendwie vernetzt ist. Dies fuhrte Aalst von der Funktion des Heuristic Miner zu der des Fuzzy Miner in dem ein flexibler Algorithmus die Wertigkeit der Vernetzungen ermittelt und mittels Edge-Filtering zu einer Abstraktion und Aggregation fuhrt. Das Tuning dieser Parameter muss jedoch Prozesstyp-spezifisch definiert werden. Es wird also noch einige Zeit dauern bis dieser Ansatz praktischen Nutzen bringen konnte.

In meiner Forschungsarbeit zu adaptiven Prozessen habe ich einen komplett anderen Ansatz gewahlt, da ich immer die menschliche Perspektive im Vordergrund sah. Mir erschien es als der logische Schritt, dass Benutzer direkt alle Elemente einer Anwendung bearbeiten konnen. Das schloss eine kompilierte Losung aus und ich entschied mich (in 1997) fur eine modellausfuhrende Technologie. Es werden alle Definitionen (Daten und Prozesse) in einem Metadaten-Objektmodell (UML-konform) erstellt und ausgefuhrt. Der typische Fachbenutzer hat jedoch keine Ahnung von Objektmodellen oder Flussdiagrammen und daher arbeitet er mit Datenelementen die aus der Geschaftsarchitektur von der IT erstellt wird. Durch diese Technologie kann ein entsprechend autorisierter Benutzer jedes Element in der Vorlage/Modell oder auch zur Laufzeit andern. Mittels SOA (oder anderen Adaptern) werden die Echtdaten eingekoppelt. Aufgrund meiner enttauschenden Erfahrungen mit Java, XML und SOA in diesem Zusammenhang, bleiben diese nur eine Option. Ich entschied ich mich fur C++, Binardaten, und freie Schnittstellenwahl.

Aber auch das Process Mining war fur mich eine interessante Option dem wir uns in 2003 zu widmen begannen. Ich stelle zur Diskussion, dass das Bild eines Prozesses nur aus der menschliche Wahrnehmung entsteht, weil unser Erinnerungsvermogen nur relativ ist und nicht absolut. Wir erinnern uns an Änderungen und Unterschiede aber kaum an absolute Werte. Daher ist ja auch das absolute Gehor nur Wenigen und mit viel Übung zuganglich. Tatsachlich besteht die Realitat aus einem komplexen Konstrukt aus Zustanden mit resonanten, kausalen Vernetzungen, die wir nur in wiederkehrenden Mustern erkennen konnen ohne deren genauen Inhalt wahrzunehmen. Flowcharts konnen dies sicher nicht realistisch abbilden. Diese Übersimplifikation ist ein Kernubel von heute marktublichem BPM in Methode und Software. Primar, weil es weder notwendig noch effizient ist. Wir erkennen aber dafur Gesichter in ganz niederer Auflosung. Wenn wir bestimmte Muster erkennen dann haben wir unser Gehirn trainiert darauf mit Aktionen (Denken oder Motorik) zu reagieren. Diese Fahigkeit wollte ich nachempfinden. Entscheidungen treffen Menschen jedoch emotional und das bleibt der Technik sicher verborgen, jedoch ist es bei Mustererkennung gar nicht notwendig das 'Warum' zu ermitteln. Man vernetzt einfach Muster und Aktion kausal. Ausfuhrbares Wissen wird nur durch Informationen vermittelt die sowohl aktuell als auch im Kontext sind. Lernen sollte man adaher auch am Besten von dem der Wissen hat. Um die Lerninhalte zu strukturieren, ist es hilfreich den Akteuren Rollen zuzuweisen und dann die Aktionen dieser Akteure zu erkennen. Dies kann ganz abstrakt auf einer freien Sammlung von Objekten mit deren Methoden erfolgen, aber es ist schneller und zielfuhrender die Strukturen einer Geschaftsarchitektur dazu zu benutzen um aus den prozess-relevanten Benutzer-Aktionen zu lernen.

Dies fuhrte zum Konzept des User-Trained Agent, also benutzertrainiertes Lernen. Dieser seit 2007 verfugbare Agent lernt nicht aus Prozesslogs der Vergangengeit, sondern in Echtzeit und macht auch in Echtzeit entsprechende Vorschlage. Er beobachtet die Aktionen einer Benutzerrolle und geht davon aus, dass die entscheidungsrelevanten Daten alle im Zustandmuster des Prozessmodells enthalten sind. Sowohl Prozess-, als auch Geschaftsdaten werden dabei als Muster erfasst. Je besser die Daten strukturiert sind umso besser und schneller erlernt der Agent was relevant ist und was nicht. Diese Technologie motiviert Unternehmen dazu ubersichtliche Daten- und Geschaftsmodelle zu erstellen. Sobald der Agent eine definierbare Anzahl von Mustern wiedererkannt hat, beginnt er Aktionen Benutzern zu empfehlen. Nimmt der Benutzer diese Vorschlage an, wird das Vertrauen des Agenten verstarkt. Wenn nicht, dann versucht er feinere Unterschiede in den Daten zu erkennen. Das Vertrauen in das eigene Wissen sinkt wieder. Der Benutzer kann auch mehrere wahrscheinliche Aktionen vorgeschlagen bekommen.

Es konnen beliebig viele Papyrus UTAs parallel und mit ubergreifenden Zustandsraumen arbeiten. Werden Prozessergebnisse in einen solchen zuruckgefuhrt (was normal ist), werden dadurch mehrere Aktionen in ihrer Sequenz voneinander abhangig da das Ergebnis der Vor-Aktion zum kausalen Muster der Folgeaktion gehort. Der tatsachliche Plan ist im Moment der Entscheidung fur den UTA nicht wirklich wichtig, sondern nur das was bisher geschah. Wenn man jedoch in den Datenobjekten auch Planungsschatzungen, Prozessregeln, sowie Zielvorgaben einschliesst, dann wird er UTA auch diese dazu verwenden um aus den Benutzeraktionen zu lernen. Setzt ein Benutzer eine bestimmte Aktion um damit eine große Diskrepanz zwischen Ziel und erwartetem Wert zu korrigieren, dann wird dies der UTA auch vorschlagen ohne zu verstehen warum. Im Machine Learning Umfeld nennt man solche Funktionen 'TRANSDUCTIVE TRAINING'. Der UTA fuhrt weder Induktion, Deduktion, noch Abstraktion von Regeln durch. Er erkennt nur wiederkehrende Muster. In kurz war das der Inhalt meines UTA-Patents von 2007.

Im Unterschied zu universitarer Forschung -- die ich nicht abtun will -- befasst sich Papyrus mit den Problemen der realen Geschaftswelt. Ich habe erst im Marz 2010 wieder auf der Uni Wien vorgetragen und bin immer erstaunt wie wenig Interesse an realitatsnahen Losungen besteht. Dabei war man den Softwareherstellern mit den Konzepten zu 'adaptiven Prozessen' um zehn Jahre voraus. Eigentlich schade!
