# Real-Time OCR and Hadoop Ingest

_Captured: 2018-06-18 at 07:12 from [dzone.com](https://dzone.com/articles/real-time-ocr-and-hadoop-ingest?edition=382200&utm_source=Daily%20Digest&utm_medium=email&utm_campaign=Daily%20Digest%202018-06-17)_

**[GitHub Source](https://github.com/tspannhw/nifi-tesseract-python)**

There are many awesome open source tools available to integrate with your Big Data Streaming flows.

Take a look at these articles for installation and why the new version of Tesseract is different.

I am officially recommending Python 3.6 or newer. Please don't use Python 2.7 if you don't have to. Friends don't let friends use old Python.

**See: [Tesseract 4 with Deep Learning](https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/)**

**[GitHub Source](https://github.com/spmallick/learnopencv/tree/master/OCR)**

For installation on a Mac Laptop:

Note: If you have Tesseract already, you may need to uninstall and unlink it first with brew. If you don't use brew, you can install another way.

## **Summary**

  1. Execute the run.sh (https://github.com/tspannhw/nifi-tesseract-python/blob/master/pytesstest.py) .
  2. It will send an MQTT message of the text and some other attributes in JSON format to the **tesseract** topic in the specified MQTT broker.
  3. Apache NiFi will read from this topic via **ConsumeMQTT.**
  4. The flow checks to see if it's valid JSON via **RouteOnContent**.
  5. We run **MergeRecord** to convert a bunch of JSON into one big Apache Avro File.
  6. Then we run **ConvertAvroToORC** to make a superfast Apache ORC file for storage.
  7. Then we store it in HDFS via **PutHDFS.**

## **Running the Python Script**

You could have this also hooked up to a scanner or point it at a directory. You could also have it scheduled to run every 30 seconds or so. I had this hooked up to a local Apache NiFi instance to schedule runs. This can also be run by MiniFi Java Agent or MiniFi C++ Agent. Or on demand, if you wish.

## **Sending MQTT Messages From Python**
    
    
         client.connect("server.server.com", 17769, 60)
    
    
         client.publish("tesseract", payload=json_string, qos=0, retain=True)

You will need to run: `pip3 install paho-mqtt`

## **Create the HDFS Directory**

## **Create the External Hive Table (DDL Built by NiFi)**

This DDL is a side effect, it's built by our ORC conversion and HDFS storage commands.

You could run that create script in Hive View 2, Beeline, or another Apache Hive JDBC/ODBC tool. I used Apache Zeppelin since I am going to be doing queries there anyway.

**Let's Ingest Our Captured Images and Process Them with Apache Tika, TensorFlow and grab the metadata**

![](https://community.hortonworks.com/storage/attachments/78401-tesseractimageprocessingflow.png)

## **Consume MQTT Records and Store in Apache Hive**

![](https://community.hortonworks.com/storage/attachments/78402-tesseractnififlow1.png)

![](https://community.hortonworks.com/storage/attachments/78427-tesseractflowend.png)

![](https://community.hortonworks.com/storage/attachments/78403-tesseractzepp2.png)

> _Let's Look at Other Fields in Zeppelin_

**Our Records in Apache Zeppelin via a SQL Query (SELECT *FROM TESSERACT)**

![](https://community.hortonworks.com/storage/attachments/78404-tesseractzeppelin.png)

**ConsumeMQTT**: Give me all the records from the **tesseract** topic from our MQTT Broker. Isolation from our ingest clients which could be 100,000 devices.

![](https://community.hortonworks.com/storage/attachments/78405-consumemqtt.png)

**MergeRecord: Merge all the JSON files sent via MQTT into one big AVRO File**

![](https://community.hortonworks.com/storage/attachments/78406-tesseractmergerecord.png)

> _ConvertAVROToORC: converts are merged AVRO file_

![](https://community.hortonworks.com/storage/attachments/78407-tesseractconvertavrotoorc.png)

**PutHDFS**

![](https://community.hortonworks.com/storage/attachments/78408-tesseractputhdfs.png)

> _Tesseract Example Schema in Hortonworks Schema Registry_

![](https://community.hortonworks.com/storage/attachments/78391-tesseractschema.png)

**TIP: **You can generate your schema with InferAvroSchema. Do that once, copy it and paste it into the Schema Registry. Then you can remove that step from your flow.

**The Schema Text**

The above schema was generated by Infer Avro Schema in Apache NiFi.

**Image Analytics Results**

This is built using a combination of Apache Tika, TensorFlow, and other metadata analysis processors.
