# 4 Challenges Faced by Artificial Intelligence

_Captured: 2017-08-30 at 09:44 from [dzone.com](https://dzone.com/articles/4-reasons-why-bad-pr-might-sink-artificial-intelli?edition=320393&utm_source=Daily%20Digest&utm_medium=email&utm_campaign=Daily%20Digest%202017-08-28)_

We've seen many buzzword-y innovations in technology over the last decade, from cloud computing to big data to microservices and beyond -- but artificial intelligence (AI) by far has the most buzzword baggage.

On one hand, AI is perhaps the most revolutionary set of innovations since the transistor. But on the other, the bad press surrounding it continues to mount, perhaps even faster than the innovations themselves.

We didn't suffer this kind of PR nightmare with the cloud, or the web, or even client/server. In fact, AI has an unprecedented set of challenges that threaten to sink the entire movement.

AI vendors, from the burgeoning gaggle of AI startups all the way to IBM, are all crowded together at the eye of this hurricane. However, this PR storm impacts enterprises as well, as AI promises to change the role technology plays in every industry on this planet.

That is, unless the killer robots do us in first.

## **Challenge #1: AI-Washing**

I'll start with the most obvious challenge (obvious because this first challenge plagues all the hot buzzwords): AI-washing. As with cloud-washing and microservices-washing, to name two of the washiest of recent memory, the "-washing" suffix reflects how vendors jump to use a new, exciting buzzword before their products truly deserve the moniker.

There's no question we're in the AI-washing phase of the AI revolution now. It seems that every vendor, from IT operations management to business intelligence to digital marketing, is now using AI under the covers. And in fact, most of them are in some fashion.

The problem isn't that these vendors are lying about what they're doing. The problem is that there are many different types of AI, from simple machine learning to much more complex deep learning and various types of cognitive computing. Today's vendors are in large part doing the easy stuff -- making it difficult for the more advanced AI vendors to rise above the noise.

## **Challenge #2: Spotty Track Record**

I remember when I was in college -- and that was a long, _long_ time ago -- AI was the hot new trend in technology circles. Long before the web was a twinkle in Tim Berners-Lee's eye, researchers heralded AI as the Next Big Thing.

Even as a child, I remember seeing on television [SHRDLU](https://en.wikipedia.org/wiki/SHRDLU): an AI program that could interpret natural language commands and execute them in a virtual environment made up of blocks of various colors and sizes. For example, you could tell SHRDLU to put the blue block on the green block, but if there were a red block on the blue one, it was smart enough to move that one out of the way first.

And then... _nothing_. Decades of nothing. Sure, we had [expert systems](https://en.wikipedia.org/wiki/Expert_system), but those were little more than rules engines executing complex sets of if-then statements. Underwhelming to be sure.

Soon, the [AI Winter](https://en.wikipedia.org/wiki/AI_winter) -- a period from roughly 1984 to 2005 where skepticism about AI led to cuts in funding for research, taking AI in a downward spiral of irrelevance -- came to pass.

However, progress did continue, slow and unnoticed, until Moore's Law and its numerous corollaries finally gave us the processor and storage horsepower we needed to actually get useful AI off the ground.

But to this day, people remember the AI Winter. Now, we're perhaps in the AI Early Spring -- but the chill is still in the air. Are we truly done with winter, or will it roll around again as it always seems to do?

## **Challenge #3: Job Killer**

Automation has been putting people out of work since the [Jacquard Loom](https://en.wikipedia.org/wiki/Jacquard_loom) helped kick off the Industrial Revolution in 1804. In the two centuries since, improvements in automation have transformed the world of employment, shifting corporate needs from repetitive manual labor to white-collar knowledge work.

Today, AI-driven assembly-line robots give us everything from mobile phones to automobiles with higher quality and at a lower real cost than ever before.

Yet while the dual forces of automation and globalization have forever transformed the role of the blue-collar labor force, the worry is that AI will do the same thing to white-collar jobs, as well.

Of course, secretaries and typing pools are mostly things of the past already. But what about financial analysts? Data analysts? Other roles that heretofore have required highly educated, intelligent people?

AI-based software vendors are racing to deliver technology that will displace such well-trained white-collar professionals -- and they're not going to slow down anytime soon.

## **Challenge #4: Killer Robots**

With the fourth challenge, the AI story diverges from all of the other buzzwordy trends in technology.

Due in large part to the impact of Hollywood and science fiction generally, AI has decidedly evil undertones. From HAL 9000 in _2001: A Space Odyssey_ to Skynet in the _Terminator_ movies, AI-run-amok as a villain is an all too common trope.

The problem with this challenge is that it holds a kernel of truth. Furthermore, separating the real dangers of AI from the Hollywood-style imagined ones is surprisingly difficult.

Nobody really knows how to teach AI right from wrong. Is the CIA using your Amazon Echo to spy on you? It's perfectly OK with that.

Or perhaps (somewhat) more realistically -- what if your babysitter says, "Alexa, deliver two large pepperoni pizzas," without your permission? You won't find out till the credit card bill arrives.

There is also bona fide concern among experts in the field that AI-coded AI programs will lead to increasingly intelligent AI-coding AI programs -- all on a Moore's Law-accelerated timeline, leading to some kind of singularity -- eventually resulting in, you guessed it, _**killer robots**_.

Science fiction? Perhaps... but just a few months ago, Microsoft announced further progress on [DeepCoder](https://qz.com/920468/artificial-intelligence-created-by-microsoft-and-university-of-cambridge-is-learning-to-write-code-by-itself-not-steal-it/), AI able to write its own software (and no, it doesn't just steal other people's code the way some articles insinuate).

There are also suspicions that Trump's "weaponized AI propaganda machine" was behind his surprising win, as [I explored in a recent article for Forbes](https://www.forbes.com/sites/jasonbloomberg/2017/03/05/does-trumps-weaponized-ai-propaganda-machine-hold-water/).

The campaign used big data tools to be sure (as all modern campaigns do), but it's not clear whether AI played a significant role -- and in any case, there was nothing stopping the Democrats from using AI, either. But if [AI starts writing fake news instead of helping us identify it](http://www.forbes.com/sites/jasonbloomberg/2017/01/22/davos-highlights-ais-massive-pr-problem/), we're all in trouble.

## **Redefine AI**

PR problems are difficult to fix because they're about perception more so than reality -- but there are a few tricks that can help move the needle.

My advice: Let's change the meaning of AI from artificial intelligence to _augmented_ intelligence.

When we say _artificial_, we mean _human-made_, as opposed to the natural intelligence that humans have. This distinction identifies two kinds of intelligence which we've now pitted against each other, battling for supremacy. This adversarial connotation to AI reinforces the _job killer_ and _killer robots_ memes in particular.

_Augmented intelligence_ tells a very different story. This phrase reinforces the fact that the only intelligence we have is the real, live, natural _human_ intelligence we've always had. AI -- no matter how smart it gets -- can only serve to augment what we all were born with.

Furthermore, _augmented_ intelligence connotes that AI is a _human_ tool -- a tool we might use for good or evil like all the other tools at our disposal, but a tool in human hands nevertheless.

And that's a good thing. Would you rather AI be a tool in human hands or the other way around?

_Copyright (C) Intellyx LLC. _ _ publishes the _ _[Agile Digital Transformation Roadmap poster_](http://www.agiledigitaltransformation.com) _, advises companies on their digital transformation initiatives, and helps vendors communicate their agility stories. As of the time of writing, none of the organizations mentioned in this article are Intellyx customers._
