# FIFO, Exactly-Once, and Other Costs

_Captured: 2017-08-14 at 10:08 from [dzone.com](https://dzone.com/articles/fifo-exactly-once-and-other-costs?edition=315392&utm_source=Daily%20Digest&utm_medium=email&utm_campaign=Daily%20Digest%202017-08-10)_

Share, secure, distribute, control, and monetize your APIs with the platform built with performance, time-to-value, and growth in mind. [Free 90-day trial](https://dzone.com/go?i=231226&u=https%3A%2F%2Fwww.redhat.com%2Fen%2Ftechnologies%2Fjboss-middleware%2F3scale%2Fget-started%3Fsc_cid%3D701f2000000h30LAAQ) of 3Scale by Red Hat

There's been [a lot](http://bravenewgeek.com/you-cannot-have-exactly-once-delivery-redux/) [of discussion](http://the-paper-trail.org/blog/exactly-not-atomic-broadcast-still-impossible-kafka/) [about](https://fpj.me/2017/07/04/no-consensus-in-exactly-once/) [exactly-once semantics](http://data.alishoker.com/2017/07/notes-on-exactly-once-semantics-in.html) [lately](https://medium.com/@jaykreps/exactly-once-one-more-time-901181d592f9), sparked by the recent [announcement](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/) of support for it in Kafka 0.11. I've already [written at length](http://bravenewgeek.com/what-you-want-is-what-you-dont-understanding-trade-offs-in-distributed-messaging/) about strong guarantees in messaging.

My former coworker Kevin Sookocheff recently made a [post](https://sookocheff.com/post/messaging/dissecting-sqs-fifo-queues/) about ordered and exactly-once message delivery as it relates to Amazon SQS. It does a good job of illustrating what the trade-offs are, and I want to drive home some points.

In the article, Kevin shows how FIFO delivery is really only meaningful when you have one single-threaded publisher and one single-threaded receiver. Amazon's FIFO queues allow you to control how restrictive this requirement is by applying ordering on a per-group basis. In other words, we can improve throughput if we can partition work into different ordered groups rather than a single totally ordered group. However, FIFO still effectively limits throughput on a group to a single publisher and single subscriber. If there are multiple publishers, they have to coordinate to ensure ordering is preserved with respect to our application's semantics. On the subscriber side, things are simpler because SQS will only deliver messages in a group one at a time in order amongst subscribers.

Amazon's FIFO queues also have an exactly-once processing feature which deduplicates messages within a five-minute window. Note, however, that there are some caveats with this, the obvious one being duplicate delivery outside of the five-minute window. A mission-critical system would have to be designed to account for this possibility. My argument here is if you still have to account for it, what's the point unless the cost of detecting duplicates is prohibitively expensive? But to be fair, five minutes probably reduces the likelihood enough to the point that it's useful and in those rare cases where it fails, the duplicate is acceptable.

The more interesting caveat is that FIFO queues do not guarantee exactly-once delivery to consumers (which, as we know, is [impossible](http://bravenewgeek.com/you-cannot-have-exactly-once-delivery/)). Rather, they offer exactly-once _processing _by guaranteeing that once a message has successfully been acknowledged as processed, it won't be delivered again. It's up to applications to ack appropriately. When a message is delivered to a consumer, it remains in the queue until it's acked. The visibility timeout prevents other consumers from processing it. With FIFO queues, this also means head-of-line blocking for other messages in the same group.

Now, let's assume a subscriber receives a batch of messages from the queue, processes them--perhaps by storing some results to a database--and then sends an acknowledgment back to SQS which removes them from the queue. It's entirely possible that during that process step a delay happens--a prolonged GC pause, crash, network delay, whatever. When this happens, the visibility timeout expires and the messages are redelivered and, potentially, reprocessed. What has to happen here is essentially cooperation between the queue and processing step. We might do this by using a database transaction to atomically process and acknowledge the messages. An alternative, yet similar, approach might be to use a write-ahead-log-like strategy whereby the consuming system reads messages from SQS and transactionally stores them in a database for future processing. Once the messages have been committed, the consumer deletes the messages from SQS. In either of these approaches, we're basically shifting the onus of exactly-once processing onto an ACID-compliant relational database.

Note that this is really how Kafka achieves its exactly-once semantics. It requires end-to-end cooperation for exactly-once to work. State changes in your application need to be committed transactionally with your Kafka offsets.

As Kevin points out, FIFO SQS queues offer exactly-once processing only if 1) publishers never publish duplicate messages wider than five minutes apart and 2) consumers never fail to delete messages they have processed from the queue. Solving either of these problems probably requires some kind of coordination between the application and queue, likely in the form of a database transaction. And if you're using a database either as the message source, sink, or both, what are exactly-once FIFO queues actually buying you? You're paying a seemingly large cost in throughput for little perceived value. Your messages are already going through some sort of transactional boundary that provides ordering and uniqueness.

Where I see FIFO and exactly-once semantics being useful is when talking to systems which cannot cooperate with the end-to-end transaction. This might be a legacy service or a system with side effects, such as sending an email. Often in the case of these "distributed workflows", latency is a lower priority and humans can be involved in various steps. Other use cases might be scheduled integrations with legacy batch processes where throughput is known a priori. These can simply be re-run when errors occur.

When people describe a messaging system with FIFO and exactly-once semantics, they're usually providing a poor description of a relational database with support for ACID transactions. Providing these semantics in a messaging system likely still involves database transactions, it's just more complicated. It turns out relational databases are really good at ensuring invariants like exactly-once.

I've picked on Kafka a bit in the past, especially with the exactly-once announcement, but my issue is not with Kafka itself. Kafka is a fantastic technology. It's well-architected, battle-tested, and the team behind it is talented and knows the space well. My issue is more with some of the intangible costs associated with it. The same goes for similar systems (like exactly-once FIFO SQS queues). Beyond just the operational complexity (which Confluent is attempting to tackle with its Kafka-as-a-service), you have to get developer understanding. This is harder than it sounds in any modestly-sized organization. That's not to say that developers are dumb or incapable of understanding, but the fact is your average developer is simply not thinking about all of the edge cases brought on by operating distributed systems at scale. They see "exactly-once FIFO queues" in SQS or "exactly-once delivery" in Kafka and take it at face value. They don't read beyond the headline. They don't look for the caveats. That's why I took issue with how Kafka claimed to do the impossible with exactly-once delivery when it's really exactly-once processing or, as I've come to call it, "atomic processing." Henry Robinson put it best when talking about the Kafka announcement:

If I were to rewrite the article, I'd structure it thus: "exactly-once looks like atomic broadcast. Atomic broadcast is impossible. Here's how exactly-once might fail, and here's why we think you shouldn't be worried about it." That's a harder argument for users to swallowâ€¦

Basically "exactly-once" markets better. It's something developers can latch onto, but it's also misleading. I know it's only a matter of time before people start linking me to the Confluent post saying, "see, exactly-once is easy!" But this is just [pain deferral](http://bravenewgeek.com/pain-driven-development-why-greedy-algorithms-are-bad-for-engineering-orgs/). On the contrary, exactly-once semantics require careful construction of your application, assume a closed, transactional world, and do not support the case where I think people want exactly-once the most: side effects.

Interestingly, one of my chief concerns about Kafka's implementation was what the difficulty of ensuring end-to-end cooperation would be in practice. Side effects into downstream systems with no support for idempotency or transactions could make it difficult. Jay's counterpoint to this was that the majority of users are using good old-fashioned relational databases, so all you really need to do is commit your offsets and state changes together. It's not trivial, but it's not that much harder than avoiding partial updates on failure if you're updating multiple tables. This brings us back to two of the original points of contention: why not merely use the database for exactly-once in the first place and what about legacy systems?

That's not to say exactly-once semantics, as offered in systems like SQS and Kafka, are not useful. I think we just need to be more conscientious of the other costs and encourage developers to more deeply understand the solution space--too much sprinkling on of Kafka or exactly-once or FIFO and not enough thinking about the actual business problem. Too much prescribing of solutions and not enough describing of problems.

Explore the core elements of owning an API strategy and best practices for effective API programs. [Download](https://dzone.com/go?i=231227&u=https%3A%2F%2Fengage.redhat.com%2F3scale-api-owners-s-201706160312%3Fsc_cid%3D701f2000000h30LAAQ) the API Owner's Manual, brought to you by 3Scale by Red Hat
